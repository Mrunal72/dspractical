{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebf4c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('gender_submission.csv')\n",
    "df.head()\n",
    "\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)\n",
    "\n",
    "summary_stats = df.describe()\n",
    "print(summary_stats)\n",
    "\n",
    "column_names = df.columns\n",
    "data_types = df.dtypes\n",
    "print(\"Column Names:\")\n",
    "print(column_names)\n",
    "print(\"Data Types:\")\n",
    "print(data_types)\n",
    "\n",
    "dimensions = df.shape\n",
    "print(\"rows, colomns\")\n",
    "print(dimensions)\n",
    "\n",
    "variable_types = df.dtypes.value_counts()\n",
    "print(variable_types)\n",
    "\n",
    "df['PassengerId'] = df['PassengerId'].astype('category')\n",
    "df['PassengerId']\n",
    "\n",
    "df_encoded = pd.get_dummies(df)\n",
    "df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3b71d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create sample dataset\n",
    "data = {\n",
    "    'Student ID': [1, 2, 3, 4, 5],\n",
    "    'Age': [18, 20, np.nan, 19, 22],\n",
    "    'Gender': ['Male', 'Female', 'Female', 'Male', 'Male'],\n",
    "    'Exam Score': [90, 85, 75, 95, 65],\n",
    "    'Study Hours': [4, 6, 8, np.nan, 5],\n",
    "    'Attendance Percentage': [80, 95, 85, 70, 60]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df\n",
    "\n",
    "\n",
    "\n",
    "print(df.isnull().sum())\n",
    "\n",
    "\n",
    "\n",
    "df['Age'].fillna(df['Age'].mean(), inplace=True)\n",
    "df['Study Hours'].fillna(df['Study Hours'].mean(), inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "print(df['Age'].value_counts())\n",
    "\n",
    "\n",
    "\n",
    "lower_bound = 65\n",
    "upper_bound = 95\n",
    "df['Exam Score'] = np.where(df['Exam Score'] < lower_bound, lower_bound, df['Exam Score'])\n",
    "df['Exam Score'] = np.where(df['Exam Score'] > upper_bound, upper_bound, df['Exam Score'])\n",
    "df['Exam Score']\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Check the distribution of 'Attendance Percentage' variable\n",
    "sns.histplot(df['Attendance Percentage'], kde=True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Apply square root transformation\n",
    "df['Attendance Percentage'] = np.sqrt(df['Attendance Percentage'])\n",
    "\n",
    "# Check the transformed distribution\n",
    "sns.histplot(df['Attendance Percentage'], kde=True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeca721",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "import pandas as pd\n",
    "\n",
    "# Read the dataset\n",
    "data = pd.read_csv('insurance.csv')\n",
    "data.head()\n",
    "\n",
    "\n",
    "\n",
    "# Calculate summary statistics grouped by a categorical variable\n",
    "categorical_variable = 'age'  # Replace with your actual column name\n",
    "numeric_variable = 'charges'  # Replace with your actual column name\n",
    "\n",
    "summary_stats = data.groupby(categorical_variable)[numeric_variable].describe()\n",
    "\n",
    "# Extract the desired statistics for each category\n",
    "desired_statistics = ['mean', 'median', 'min', 'max', 'std']\n",
    "\n",
    "# Create a list of numeric values for each response to the categorical variable\n",
    "category_values = summary_stats.index.tolist()\n",
    "\n",
    "# Print the summary statistics\n",
    "print(summary_stats)\n",
    "print(\"Category values:\", category_values)\n",
    "\n",
    "\n",
    "#2 part\n",
    "# Download dataset from https://www.kaggle.com/datasets/uciml/iris\n",
    "\n",
    "# !pip install pandas numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('/content/Iris.csv')\n",
    "df.head()\n",
    "\n",
    "\n",
    "# Filter dataset for 'Iris-setosa', 'Iris-versicolor', and 'Iris-virginica'\n",
    "setosa_df = df[df['Species'] == 'Iris-setosa']\n",
    "versicolor_df = df[df['Species'] == 'Iris-versicolor']\n",
    "virginica_df = df[df['Species'] == 'Iris-virginica']\n",
    "\n",
    "\n",
    "\n",
    "# Calculate statistical details for each species\n",
    "setosa_stats = setosa_df.describe()\n",
    "versicolor_stats = versicolor_df.describe()\n",
    "virginica_stats = virginica_df.describe()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Statistical details for 'Iris-setosa':\")\n",
    "print(setosa_stats)\n",
    "\n",
    "print(\"\\nStatistical details for 'Iris-versicolor':\")\n",
    "print(versicolor_stats)\n",
    "\n",
    "print(\"\\nStatistical details for 'Iris-virginica':\")\n",
    "print(virginica_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f446c373",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 \n",
    "#pip install pandas numpy scikit-learn matplotlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('boston_housing.csv')\n",
    "df.head()\n",
    "\n",
    "\n",
    "#Split the dataset into input features and target variable\n",
    "X = df.drop('medv', axis=1)\n",
    "y = df['medv']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Create and train the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate mean squared error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Calculate coefficient of determination (R^2)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Coefficient of Determination (R^2):\", r2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example: Predict home prices for new_data\n",
    "new_data = pd.DataFrame([1])\n",
    "predicted_prices = model.predict(new_data)\n",
    "print(\"Predicted prices:\", predicted_prices)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918c3791",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "# Download dataset from https://www.kaggle.com/datasets/rakeshrau/social-network-ads\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Social_Network_Ads.csv\")\n",
    "data.head()\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# Separate the features (X) and the target variable (y)\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "# Perform label encoding on the 'Gender' column\n",
    "le = LabelEncoder()\n",
    "X[:, 1] = le.fit_transform(X[:, 1])\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an instance of the Logistic Regression model\n",
    "logistic_regression = LogisticRegression()\n",
    "\n",
    "# Train the model on the training data\n",
    "logistic_regression.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = logistic_regression.predict(X_test)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Extract the values from the confusion matrix\n",
    "TN = confusion[0, 0]  # True Negative\n",
    "FP = confusion[0, 1]  # False Positive\n",
    "FN = confusion[1, 0]  # False Negative\n",
    "TP = confusion[1, 1]  # True Positive\n",
    "\n",
    "# Compute the accuracy\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "# Compute the error rate\n",
    "error_rate = (FP + FN) / (TP + TN + FP + FN)\n",
    "\n",
    "# Compute the precision\n",
    "precision = TP / (TP + FP)\n",
    "\n",
    "# Compute the recall\n",
    "recall = TP / (TP + FN)\n",
    "\n",
    "# display the confusion matrix\n",
    "print(confusion)\n",
    "\n",
    "# display the accuracy\n",
    "print(accuracy)\n",
    "\n",
    "# display the error rate\n",
    "print(error_rate)\n",
    "\n",
    "# display the precision\n",
    "print(precision)\n",
    "\n",
    "# display the recall\n",
    "print(recall)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b842a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6\n",
    "\n",
    "# import libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('Iris.csv')\n",
    "\n",
    "# Split the dataset into features and labels\n",
    "X = df.drop('Species', axis=1)\n",
    "y = df['Species']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Gaussian Naive Bayes classifier\n",
    "classifier = GaussianNB()\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mat)\n",
    "\n",
    "# Extract TN, FP, FN, TP from the confusion matrix\n",
    "tn, fp, fn, tp = confusion_mat[0, 0], confusion_mat[0, 1], confusion_mat[1, 0], confusion_mat[1, 1]\n",
    "\n",
    "# Compute evaluation metrics\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "error_rate = 1 - accuracy\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Error Rate:\", error_rate)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ff7f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7\n",
    "document = \"\"\"\n",
    "Natural language processing (NLP) is a subfield of artificial intelligence (AI) that focuses on the interaction between computers and humans using natural language. It involves the analysis, understanding, and generation of human language, enabling machines to process and comprehend text in a meaningful way. NLP techniques are widely used in various applications such as sentiment analysis, machine translation, chatbots, and information retrieval. Preprocessing is an essential step in NLP, which involves tokenization, part-of-speech tagging, stop words removal, stemming, and lemmatization.\n",
    "\"\"\"\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(document)\n",
    "\n",
    "# POS Tagging\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Stop words removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "\n",
    "# Print the results\n",
    "print(\"Original Document:\\n\", document)\n",
    "print(\"\\nTokens:\\n\", tokens)\n",
    "print(\"\\nPOS Tags:\\n\", pos_tags)\n",
    "print(\"\\nFiltered Tokens (after stop words removal):\\n\", filtered_tokens)\n",
    "print(\"\\nStemmed Tokens:\\n\", stemmed_tokens)\n",
    "print(\"\\nLemmatized Tokens:\\n\", lemmatized_tokens)\n",
    "\n",
    "#part2\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# List of documents\n",
    "documents = [\n",
    "    \"Natural language processing is a subfield of artificial intelligence.\",\n",
    "    \"It focuses on the interaction between computers and humans using natural language.\",\n",
    "    \"NLP techniques are widely used in various applications such as sentiment analysis and machine translation.\",\n",
    "    \"Preprocessing is an essential step in NLP.\",\n",
    "]\n",
    "\n",
    "# Create an instance of TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get the feature names (terms)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the TF-IDF representation\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"Document {i+1}:\")\n",
    "    for j, term in enumerate(feature_names):\n",
    "        tfidf_value = tfidf_matrix[i, j]\n",
    "        if tfidf_value > 0:\n",
    "            print(f\"{term}: {tfidf_value:.4f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c8f36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the Titanic dataset\n",
    "df = sns.load_dataset('titanic')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Countplot:\n",
    "sns.countplot(x='survived', data=df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Barplot:\n",
    "sns.barplot(x='sex', y='survived', hue='class', data=df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " #Heatmap:\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
    "\n",
    "\n",
    "#part2\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Titanic dataset\n",
    "df = sns.load_dataset('titanic')\n",
    "\n",
    "# Plot histogram of ticket prices\n",
    "sns.histplot(data=df, x='fare', kde=True)\n",
    "\n",
    "# Set plot title and labels\n",
    "plt.title('Distribution of Ticket Prices')\n",
    "plt.xlabel('Ticket Fare')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ccefa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Load the 'titanic' dataset\n",
    "titanic_df = sns.load_dataset('titanic')\n",
    "\n",
    "# Filter out the necessary columns\n",
    "data = titanic_df[['sex', 'age', 'survived']]\n",
    "\n",
    "# Remove rows with missing age values\n",
    "data = data.dropna(subset=['age'])\n",
    "\n",
    "# Create a box plot using seaborn\n",
    "sns.boxplot(x='sex', y='age', hue='survived', data=data)\n",
    "\n",
    "# Set the plot title and labels\n",
    "plt.title('Distribution of Age with respect to Gender and Survival')\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('Age')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9232a95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download the Iris dataset from the provided URL\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "iris_df = pd.read_csv(url, header=None, names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class'])\n",
    "iris_df.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# List down the features and their types\n",
    "features = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "feature_types = ['numeric', 'numeric', 'numeric', 'numeric']\n",
    "\n",
    "# Print the features and their types\n",
    "for feature, ftype in zip(features, feature_types):\n",
    "    print(f\"Feature: {feature} - Type: {ftype}\")\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# Create histograms for each feature\n",
    "iris_df[features].hist()\n",
    "plt.suptitle('Histograms of Iris Dataset Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create box plots for each feature\n",
    "iris_df[features].plot(kind='box')\n",
    "plt.title('Box Plots of Iris Dataset Features')\n",
    "plt.show()\n",
    "\n",
    "# Identify outliers\n",
    "outliers = []\n",
    "for feature in features:\n",
    "    q1 = iris_df[feature].quantile(0.25)\n",
    "    q3 = iris_df[feature].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    feature_outliers = iris_df[(iris_df[feature] < lower_bound) | (iris_df[feature] > upper_bound)]\n",
    "    outliers.append(feature_outliers)\n",
    "\n",
    "# Print the outliers for each feature\n",
    "for feature, outlier_df in zip(features, outliers):\n",
    "    print(f\"\\nOutliers for feature: {feature}\")\n",
    "    print(outlier_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
